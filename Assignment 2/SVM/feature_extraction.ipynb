{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'..')\n",
    "from data import read_svm_data\n",
    "from cvxopt import matrix, solvers, spmatrix\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_std = np.std(X, axis=0)\n",
    "    X_std[X_std == 0] = 1\n",
    "    X_standardized = (X - X_mean) / X_std\n",
    "    return X_standardized\n",
    "\n",
    "def cov(X):\n",
    "    m = X.shape[0]\n",
    "    covariance_matrix = np.dot(X.T, X) / m\n",
    "    return covariance_matrix\n",
    "\n",
    "def get_eigenvectors(X, top_n):\n",
    "    X_standardized = standardize(X)\n",
    "    covariance_matrix = cov(X_standardized)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    sorted_idx = np.argsort(eigenvalues)[::-1]\n",
    "    selected_eigenvectors = eigenvectors[:, sorted_idx[:top_n]]\n",
    "    return selected_eigenvectors, np.mean(X, axis=0), np.std(X, axis=0)\n",
    "\n",
    "def pca(X, components, mean, std):\n",
    "    std[std == 0] = 1\n",
    "    X_standardized = (X - mean) / std\n",
    "    return np.dot(X_standardized, components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 784), (20000,), (3974, 784), (3974,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels, training_images = read_svm_data(\"training\", r\"../../MNIST_ORG\", [2, 3, 8, 9])\n",
    "testing_labels, testing_images = read_svm_data(\"testing\", r\"../../MNIST_ORG\", [2, 3, 8, 9])\n",
    "\n",
    "training_images.shape, training_labels.shape, testing_images.shape, testing_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "components, train_mean, train_std = get_eigenvectors(training_images, 10)\n",
    "training_images = pca(training_images, components, train_mean, train_std)\n",
    "testing_images = pca(testing_images, components, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1\n",
    "N = training_labels.shape[0]  # number of training samples\n",
    "d = training_images.shape[1]  # dimension of each sample\n",
    "labels_to_classify = [2, 3, 8, 9]\n",
    "classifiers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for digit 2...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.4873e+04  6.8384e+04  5e+05  6e+00  2e+03\n",
      " 1:  3.7203e+04 -4.1980e+04  1e+05  8e-01  2e+02\n",
      " 2:  1.5461e+04 -5.6921e+03  2e+04  2e-01  5e+01\n",
      " 3:  6.2069e+03  4.7036e+02  6e+03  4e-02  1e+01\n",
      " 4:  3.8406e+03  1.9708e+03  2e+03  1e-02  3e+00\n",
      " 5:  3.5858e+03  2.1552e+03  2e+03  6e-03  2e+00\n",
      " 6:  3.3347e+03  2.3065e+03  1e+03  4e-03  1e+00\n",
      " 7:  3.1702e+03  2.4033e+03  8e+02  3e-03  8e-01\n",
      " 8:  3.0601e+03  2.4665e+03  6e+02  2e-03  6e-01\n",
      " 9:  2.9862e+03  2.5089e+03  5e+02  1e-03  4e-01\n",
      "10:  2.9008e+03  2.5564e+03  4e+02  8e-04  2e-01\n",
      "11:  2.8605e+03  2.5773e+03  3e+02  6e-04  2e-01\n",
      "12:  2.8152e+03  2.6039e+03  2e+02  4e-04  1e-01\n",
      "13:  2.7810e+03  2.6238e+03  2e+02  2e-04  7e-02\n",
      "14:  2.7604e+03  2.6364e+03  1e+02  2e-04  5e-02\n",
      "15:  2.7416e+03  2.6475e+03  1e+02  9e-05  3e-02\n",
      "16:  2.7227e+03  2.6604e+03  6e+01  5e-05  1e-02\n",
      "17:  2.7136e+03  2.6671e+03  5e+01  3e-05  9e-03\n",
      "18:  2.7055e+03  2.6730e+03  3e+01  2e-05  5e-03\n",
      "19:  2.6981e+03  2.6787e+03  2e+01  8e-06  2e-03\n",
      "20:  2.6951e+03  2.6810e+03  1e+01  4e-06  1e-03\n",
      "21:  2.6908e+03  2.6848e+03  6e+00  1e-06  3e-04\n",
      "22:  2.6901e+03  2.6853e+03  5e+00  6e-07  2e-04\n",
      "23:  2.6890e+03  2.6864e+03  3e+00  3e-07  8e-05\n",
      "24:  2.6883e+03  2.6870e+03  1e+00  1e-07  3e-05\n",
      "25:  2.6879e+03  2.6874e+03  5e-01  7e-16  6e-12\n",
      "26:  2.6877e+03  2.6876e+03  2e-01  6e-16  3e-11\n",
      "27:  2.6877e+03  2.6876e+03  1e-01  6e-16  2e-10\n",
      "28:  2.6877e+03  2.6876e+03  6e-02  7e-16  1e-10\n",
      "29:  2.6877e+03  2.6876e+03  1e-02  7e-16  4e-10\n",
      "30:  2.6877e+03  2.6877e+03  2e-04  8e-16  2e-10\n",
      "Terminated (maximum number of iterations reached).\n",
      "Training classifier for digit 3...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.3880e+04  6.1117e+04  4e+05  5e+00  1e+03\n",
      " 1:  3.1475e+04 -3.0372e+04  8e+04  8e-01  2e+02\n",
      " 2:  1.7860e+04 -8.9418e+03  3e+04  3e-01  7e+01\n",
      " 3:  1.3246e+04 -4.3681e+03  2e+04  2e-01  4e+01\n",
      " 4:  8.3884e+03 -1.3523e+02  1e+04  8e-02  2e+01\n",
      " 5:  5.9100e+03  1.8577e+03  4e+03  3e-02  7e+00\n",
      " 6:  4.6596e+03  2.7037e+03  2e+03  1e-02  3e+00\n",
      " 7:  4.2483e+03  2.9713e+03  1e+03  7e-03  2e+00\n",
      " 8:  3.9942e+03  3.1250e+03  9e+02  4e-03  1e+00\n",
      " 9:  3.8946e+03  3.1847e+03  7e+02  3e-03  7e-01\n",
      "10:  3.7439e+03  3.2717e+03  5e+02  2e-03  4e-01\n",
      "11:  3.6443e+03  3.3303e+03  3e+02  9e-04  2e-01\n",
      "12:  3.5783e+03  3.3688e+03  2e+02  5e-04  1e-01\n",
      "13:  3.5371e+03  3.3942e+03  1e+02  3e-04  8e-02\n",
      "14:  3.5099e+03  3.4108e+03  1e+02  2e-04  4e-02\n",
      "15:  3.4901e+03  3.4229e+03  7e+01  9e-05  2e-02\n",
      "16:  3.4785e+03  3.4309e+03  5e+01  6e-05  1e-02\n",
      "17:  3.4656e+03  3.4402e+03  3e+01  2e-05  5e-03\n",
      "18:  3.4586e+03  3.4456e+03  1e+01  5e-06  1e-03\n",
      "19:  3.4558e+03  3.4480e+03  8e+00  2e-06  4e-04\n",
      "20:  3.4540e+03  3.4497e+03  4e+00  7e-07  2e-04\n",
      "21:  3.4534e+03  3.4502e+03  3e+00  3e-07  8e-05\n",
      "22:  3.4525e+03  3.4511e+03  1e+00  1e-07  3e-05\n",
      "23:  3.4519e+03  3.4516e+03  4e-01  2e-08  5e-06\n",
      "24:  3.4518e+03  3.4517e+03  1e-01  6e-09  2e-06\n",
      "25:  3.4518e+03  3.4517e+03  3e-02  1e-09  3e-07\n",
      "26:  3.4518e+03  3.4518e+03  1e-02  3e-10  7e-08\n",
      "27:  3.4518e+03  3.4518e+03  1e-04  3e-12  5e-10\n",
      "Optimal solution found.\n",
      "Training classifier for digit 8...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.1904e+04  6.8781e+04  5e+05  6e+00  1e+03\n",
      " 1:  3.8641e+04 -3.6950e+04  1e+05  8e-01  2e+02\n",
      " 2:  1.9215e+04 -3.5750e+03  3e+04  2e-01  3e+01\n",
      " 3:  8.3609e+03  3.1830e+03  6e+03  3e-02  7e+00\n",
      " 4:  6.6158e+03  4.3795e+03  2e+03  1e-02  2e+00\n",
      " 5:  6.1653e+03  4.7051e+03  2e+03  7e-03  1e+00\n",
      " 6:  5.8950e+03  4.8843e+03  1e+03  4e-03  9e-01\n",
      " 7:  5.7225e+03  4.9959e+03  8e+02  3e-03  5e-01\n",
      " 8:  5.6004e+03  5.0740e+03  6e+02  2e-03  4e-01\n",
      " 9:  5.5044e+03  5.1353e+03  4e+02  1e-03  2e-01\n",
      "10:  5.4466e+03  5.1726e+03  3e+02  8e-04  1e-01\n",
      "11:  5.3791e+03  5.2155e+03  2e+02  3e-04  6e-02\n",
      "12:  5.3506e+03  5.2355e+03  1e+02  2e-04  4e-02\n",
      "13:  5.3213e+03  5.2565e+03  7e+01  1e-04  2e-02\n",
      "14:  5.3042e+03  5.2692e+03  4e+01  5e-05  9e-03\n",
      "15:  5.2946e+03  5.2766e+03  2e+01  2e-05  4e-03\n",
      "16:  5.2894e+03  5.2808e+03  9e+00  8e-06  2e-03\n",
      "17:  5.2871e+03  5.2826e+03  5e+00  3e-06  6e-04\n",
      "18:  5.2855e+03  5.2839e+03  2e+00  2e-07  4e-05\n",
      "19:  5.2849e+03  5.2845e+03  4e-01  4e-08  7e-06\n",
      "20:  5.2848e+03  5.2846e+03  2e-01  1e-08  2e-06\n",
      "21:  5.2847e+03  5.2847e+03  3e-02  2e-09  4e-07\n",
      "22:  5.2847e+03  5.2847e+03  4e-04  2e-11  4e-09\n",
      "Optimal solution found.\n",
      "Training classifier for digit 9...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5153e+04  5.3114e+04  3e+05  5e+00  1e+03\n",
      " 1:  2.4611e+04 -2.3538e+04  6e+04  7e-01  2e+02\n",
      " 2:  1.0285e+04 -5.0699e+03  2e+04  2e-01  5e+01\n",
      " 3:  6.3083e+03 -1.8316e+03  1e+04  9e-02  3e+01\n",
      " 4:  4.3560e+03 -1.8219e+02  5e+03  4e-02  1e+01\n",
      " 5:  3.0745e+03  8.6361e+02  2e+03  2e-02  6e+00\n",
      " 6:  2.4041e+03  1.3832e+03  1e+03  8e-03  2e+00\n",
      " 7:  2.2563e+03  1.5012e+03  8e+02  5e-03  1e+00\n",
      " 8:  2.1112e+03  1.6099e+03  5e+02  3e-03  7e-01\n",
      " 9:  2.0468e+03  1.6542e+03  4e+02  2e-03  5e-01\n",
      "10:  1.9908e+03  1.6918e+03  3e+02  1e-03  4e-01\n",
      "11:  1.9486e+03  1.7191e+03  2e+02  9e-04  2e-01\n",
      "12:  1.9133e+03  1.7413e+03  2e+02  5e-04  1e-01\n",
      "13:  1.8914e+03  1.7560e+03  1e+02  4e-04  1e-01\n",
      "14:  1.8702e+03  1.7700e+03  1e+02  2e-04  7e-02\n",
      "15:  1.8551e+03  1.7800e+03  8e+01  1e-04  4e-02\n",
      "16:  1.8431e+03  1.7883e+03  6e+01  8e-05  2e-02\n",
      "17:  1.8300e+03  1.7969e+03  3e+01  2e-05  4e-03\n",
      "18:  1.8257e+03  1.8007e+03  3e+01  1e-05  3e-03\n",
      "19:  1.8223e+03  1.8034e+03  2e+01  6e-06  2e-03\n",
      "20:  1.8188e+03  1.8065e+03  1e+01  3e-06  7e-04\n",
      "21:  1.8158e+03  1.8092e+03  7e+00  8e-07  2e-04\n",
      "22:  1.8142e+03  1.8107e+03  3e+00  2e-07  6e-05\n",
      "23:  1.8133e+03  1.8116e+03  2e+00  7e-08  2e-05\n",
      "24:  1.8126e+03  1.8123e+03  3e-01  1e-08  3e-06\n",
      "25:  1.8125e+03  1.8124e+03  6e-02  2e-09  5e-07\n",
      "26:  1.8124e+03  1.8124e+03  1e-02  3e-10  9e-08\n",
      "27:  1.8124e+03  1.8124e+03  1e-04  3e-12  7e-10\n",
      "Optimal solution found.\n",
      "Part A Training Accuracy: 88.97%\n",
      "Part A Testing Accuracy: 89.38%\n"
     ]
    }
   ],
   "source": [
    "N = training_labels.shape[0]\n",
    "d = training_images.shape[1]\n",
    "\n",
    "Q_rows = []\n",
    "Q_cols = []\n",
    "Q_vals = []\n",
    "\n",
    "# identity for w part\n",
    "for i in range(d):\n",
    "    Q_rows.append(i)\n",
    "    Q_cols.append(i)\n",
    "    Q_vals.append(1.0)\n",
    "\n",
    "# ensuring that the slack variables' matrix is positive semi-definite (otherwise cvxopt raises an error)\n",
    "for i in range(N):\n",
    "    Q_rows.append(d + 1 + i)\n",
    "    Q_cols.append(d + 1 + i)\n",
    "    Q_vals.append(1e-6)\n",
    "\n",
    "# sparse Q matrix\n",
    "Q = spmatrix(Q_vals, Q_rows, Q_cols, (d + N + 1, d + N + 1), 'd')\n",
    "\n",
    "# p vector\n",
    "p = matrix([0.0] * (d + 1) + [C] * N)\n",
    "\n",
    "\n",
    "solvers.options['maxiters'] = 30\n",
    "\n",
    "for label in labels_to_classify:\n",
    "    print(f\"Training classifier for digit {label}...\")\n",
    "    yn = np.where(training_labels == label, 1, -1)\n",
    "    \n",
    "    A_rows, A_cols, A_vals = [], [], []\n",
    "\n",
    "    # constructing sparse A matrix for the constraint y_i(w*x_i​ + b) >= 1 − e_i\n",
    "    # converted to −y_i(w*x_i + b) − e_i <= −1, which is Gx <= h form\n",
    "    for i in range(N):\n",
    "        # -y_i * x_i\n",
    "        for j in range(d):\n",
    "            if training_images[i, j] != 0:\n",
    "                A_rows.append(i)\n",
    "                A_cols.append(j)\n",
    "                A_vals.append(float(-yn[i] * training_images[i, j]))\n",
    "        # -y_i\n",
    "        A_rows.append(i)\n",
    "        A_cols.append(d)\n",
    "        A_vals.append(float(-yn[i]))\n",
    "        \n",
    "        # slack variable\n",
    "        A_rows.append(i)\n",
    "        A_cols.append(d + 1 + i)\n",
    "        A_vals.append(-1.0)\n",
    "\n",
    "    # constraint e_i >= 0\n",
    "    for i in range(N):\n",
    "        A_rows.append(N + i)\n",
    "        A_cols.append(d + 1 + i)\n",
    "        A_vals.append(-1.0)\n",
    "\n",
    "    A = spmatrix(A_vals, A_rows, A_cols, (N + N, d + N + 1))\n",
    "\n",
    "    # c vector\n",
    "    c = matrix([-1.0] * N + [0.0] * N)\n",
    "    \n",
    "    # solve\n",
    "    sol = solvers.qp(Q, p, A, c)\n",
    "\n",
    "    w = np.array(sol['x'][:d]).flatten()\n",
    "    b = sol['x'][d]\n",
    "\n",
    "    classifiers[label] = (w, b)\n",
    "    \n",
    "def predict(X, classifiers):\n",
    "    predictions = {label: np.dot(X, w) + b for label, (w, b) in classifiers.items()}\n",
    "    final_predictions = np.fromiter((max(predictions, key=lambda x: predictions[x][i]) for i in range(len(X))), dtype=int)\n",
    "    return final_predictions\n",
    "\n",
    "predictions = predict(training_images, classifiers)\n",
    "mapped_labels = np.array([label if label in labels_to_classify else None for label in training_labels])\n",
    "correct_predictions = np.sum(predictions == mapped_labels)\n",
    "accuracy = correct_predictions / len(training_labels)\n",
    "print(f\"Part A Training Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "predictions = predict(testing_images, classifiers)\n",
    "mapped_labels = np.array([label if label in labels_to_classify else None for label in testing_labels])\n",
    "correct_predictions = np.sum(predictions == mapped_labels)\n",
    "accuracy = correct_predictions / len(testing_labels)\n",
    "print(f\"Part A Testing Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training classifier for class 2\n",
      "[LibSVM]\n",
      "Training classifier for class 3\n",
      "[LibSVM]\n",
      "Training classifier for class 8\n",
      "[LibSVM]\n",
      "Training classifier for class 9\n",
      "[LibSVM]Training Accuracy: 89.10%\n",
      "Test Accuracy: 89.13%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for class_label in [2,3,8,9]:\n",
    "    print(f\"\\nTraining classifier for class {class_label}\")\n",
    "    # 1 for the current class, 0 for all others\n",
    "    binary_target = (training_labels == class_label).astype(int)\n",
    "    \n",
    "    classifier = SVC(kernel='linear', C=1, verbose=True)\n",
    "    classifier.fit(training_images, binary_target)\n",
    "    classifiers[class_label] = classifier\n",
    "    \n",
    "def predict_one_vs_rest(classifiers, images):\n",
    "    scores = np.column_stack([\n",
    "        clf.decision_function(images) for clf in classifiers.values()\n",
    "    ])\n",
    "    predicted_class_indices = np.argmax(scores, axis=1)\n",
    "    predicted_classes = [[2,3,8,9][i] for i in predicted_class_indices]\n",
    "    \n",
    "    return predicted_classes\n",
    "\n",
    "predictions = predict_one_vs_rest(classifiers, training_images)\n",
    "accuracy = accuracy_score(training_labels, predictions)\n",
    "print(f\"Training Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "predictions = predict_one_vs_rest(classifiers, testing_images)\n",
    "accuracy = accuracy_score(testing_labels, predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training classifier for digit 2...\n",
      "Building Kernel matrix...\n",
      "Solving QP...\n",
      "\n",
      "Training classifier for digit 3...\n",
      "Building Kernel matrix...\n",
      "Solving QP...\n",
      "\n",
      "Training classifier for digit 8...\n",
      "Building Kernel matrix...\n",
      "Solving QP...\n",
      "\n",
      "Training classifier for digit 9...\n",
      "Building Kernel matrix...\n",
      "Solving QP...\n",
      "Part C Training Accuracy: 93.11%\n",
      "Part C Testing Accuracy: 92.98%\n"
     ]
    }
   ],
   "source": [
    "def polynomial_kernel(X1, X2, degree=2):\n",
    "    K = np.dot(X1, X2.T)\n",
    "    return (1 + K) ** degree\n",
    "\n",
    "solvers.options['show_progress'] = False\n",
    "\n",
    "N = training_images[::10].shape[0]\n",
    "\n",
    "\n",
    "for label in labels_to_classify:\n",
    "    print(f\"\\nTraining classifier for digit {label}...\")\n",
    "    yn = np.where(training_labels[::10] == label, 1, -1)\n",
    "\n",
    "    print(\"Building Kernel matrix...\")\n",
    "    \n",
    "    # Q is a NxN matrix, where N is the number of training samples\n",
    "    # Q[i, j] = y[i] * y[j] * K(x[i], x[j])\n",
    "    K = polynomial_kernel(training_images[::10], training_images[::10])\n",
    "    Y = yn.reshape(-1, 1) * yn.reshape(1, -1)\n",
    "    Q = matrix(K * Y, tc='d')\n",
    "\n",
    "    # p is a Nx1 matrix of -1s\n",
    "    p = matrix(-np.ones(N))\n",
    "    \n",
    "    # Ax = c equality constraint sum(alpha * y) = 0\n",
    "    # A is a row vector of labels y\n",
    "    # c is a scalar 0\n",
    "    \n",
    "    A = matrix(yn, (1, N), 'd')\n",
    "    c = matrix(0.0)\n",
    "    \n",
    "    # Gx <= h inequality constraint 0 <= alpha <= C\n",
    "    # lower bound 0 <= alpha\n",
    "    G = spmatrix([], [], [], (N, N), 'd')\n",
    "    G[::N+1] = -1\n",
    "    h = matrix(np.zeros(N), tc='d')\n",
    "    \n",
    "    # upper bound alpha <= C\n",
    "    G_up = spmatrix([], [], [], (N, N), 'd')\n",
    "    G_up[::N+1] = 1\n",
    "    h_up = matrix(C * np.ones(N), tc='d')\n",
    "    \n",
    "    # concatenate lower and upper bound\n",
    "    G = matrix([G, G_up])\n",
    "    h = matrix([h, h_up])\n",
    "\n",
    "    print(\"Solving QP...\")\n",
    "    solution = solvers.qp(Q, p, G, h, A, c)\n",
    "    alphas = np.array(solution['x']).flatten()\n",
    "\n",
    "    # support vectors have non zero lagrange multipliers\n",
    "    # alphas > 0 caused numerical problems, so we use 1e-6\n",
    "    sv = alphas > 1e-6\n",
    "    ind = np.arange(len(alphas))[sv]    # indices of support vectors\n",
    "    alpha_sv = alphas[sv]               # alphas of support vectors\n",
    "    sv_y = yn[sv]                       # labels of support vectors\n",
    "    sv_X = training_images[::10][sv]      # support vectors\n",
    "\n",
    "    # b = 1/N * sum(y - sum(alpha * y * K))\n",
    "    b = np.mean(sv_y - np.sum(alpha_sv * sv_y * K[np.ix_(ind, sv)], axis=0))\n",
    "\n",
    "    classifiers[label] = (alpha_sv, sv_X, sv_y, b)\n",
    "    \n",
    "def predict(X, classifiers):\n",
    "    results = {}\n",
    "    for label, (alphas, support_vectors, sv_labels, b) in classifiers.items():\n",
    "        K_eval = polynomial_kernel(X, support_vectors)\n",
    "        prediction = np.dot(K_eval, alphas * sv_labels) + b\n",
    "        results[label] = prediction\n",
    "    predictions = np.argmax(np.column_stack([results[label] for label in labels_to_classify]), axis=1)\n",
    "    mapped_labels = [labels_to_classify[i] for i in predictions]\n",
    "    return np.array(mapped_labels)\n",
    "\n",
    "# Prediction and accuracy calculation\n",
    "predictions = predict(training_images, classifiers)\n",
    "accuracy = np.mean(predictions == training_labels)\n",
    "print(f\"Part C Training Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "predictions = predict(testing_images, classifiers)\n",
    "accuracy = np.mean(predictions == testing_labels)\n",
    "print(f\"Part C Testing Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training classifier for class 2\n",
      "[LibSVM]\n",
      "Training classifier for class 3\n",
      "[LibSVM]\n",
      "Training classifier for class 8\n",
      "[LibSVM]\n",
      "Training classifier for class 9\n",
      "[LibSVM]Training Accuracy: 93.64%\n",
      "Test Accuracy: 93.28%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifiers = {}\n",
    "classes = [2, 3, 8, 9]\n",
    "for class_label in classes:\n",
    "    print(f\"\\nTraining classifier for class {class_label}\")\n",
    "    # 1 for the current class, 0 for all others\n",
    "    binary_target = (training_labels == class_label).astype(int)\n",
    "    \n",
    "    classifier = SVC(kernel='poly', C=1, verbose=True)\n",
    "    classifier.fit(training_images, binary_target)\n",
    "    classifiers[class_label] = classifier\n",
    "\n",
    "def predict_one_vs_rest(classifiers, images):\n",
    "    scores = np.column_stack([\n",
    "        clf.decision_function(images) for clf in classifiers.values()\n",
    "    ])\n",
    "    predicted_class_indices = np.argmax(scores, axis=1)\n",
    "    predicted_classes = [classes[i] for i in predicted_class_indices]\n",
    "    \n",
    "    return predicted_classes\n",
    "\n",
    "predictions = predict_one_vs_rest(classifiers, training_images)\n",
    "accuracy = accuracy_score(training_labels, predictions)\n",
    "print(f\"Training Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "predictions = predict_one_vs_rest(classifiers, testing_images)\n",
    "accuracy = accuracy_score(testing_labels, predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
